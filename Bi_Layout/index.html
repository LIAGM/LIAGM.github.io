<!DOCTYPE html>
<html>

<head>
  <!-- Google tag (gtag.js) -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-YNMMXDB9CH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-YNMMXDB9CH');
  </script> -->
  <meta charset="utf-8">
  <meta name="description" content="No More Ambiguity in 360° Room Layout via Bi-Layout Estimation">
  <meta name="keywords" content="Panorama, Room Layout, Layout Estimation, Ambiguity">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>No More Ambiguity in 360° Room Layout via Bi-Layout Estimation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://liagm.github.io/" target="_blank">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://liagm.github.io/DAEFR/">
            DAEFR
          </a>
          <!-- <a class="navbar-item" href="https://github.com/amazon-science/read-up">
            READ-UP
          </a>
          <a class="navbar-item" href="https://ttaoretw.github.io/SeqRQ-AE/demo.html">
            Towards Unsupervised ASR and TTS
          </a>
          <a class="navbar-item" href="https://henryhenrychen.github.io/CL-transfer-demo/">
            Cross-lingual Transfer Linearing for TTS
          </a> -->
          </div>
        </div>
      </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">No More Ambiguity in 360° Room Layout via<br> Bi-Layout Estimation</h1>
            <h2 class="title is-size-3 publication-title">CVPR 2024</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://liagm.github.io/" target="_blank">Yu-Ju Tsai</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://aliensunmin.github.io/lab/info.html" target="_blank">Jin-Cheng Jhang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/jingjing-zheng-1447b427/" target="_blank">Jingjing Zheng</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.amazon.science/author/wei-wang" target="_blank">Wei Wang</a><sup>3</sup>,
              </span>
              <br>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/aycchen/" target="_blank">Albert Y. C. Chen</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://aliensunmin.github.io/" target="_blank">Min Sun</a><sup>2,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/cheng-hao-kuo-65208353/" target="_blank">Cheng-Hao Kuo</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://faculty.ucmerced.edu/mhyang/" target="_blank">Ming-Hsuan Yang</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>UC Merced,</span>
              <span class="author-block"><sup>2</sup>National Tsing Hua University,</span>
              <span class="author-block"><sup>3</sup>Amazon</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="pdf/3739.pdf" class="external-link button is-normal is-rounded is-dark"
                    target="_blank">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- arXiv Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark"
                    target="_blank">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/LIAGM/DAEFR" class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code(coming soon)</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="pdf/3739_supp.pdf"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Real world results with slider -->
  <section class="section">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">Our Bi-Layout Estimations</h2>
      <p><strong><font color="Blue">Blue</font></strong> and <strong><font color="#82FF82">Green</font></strong> represent ground truth labels and our predictions, respectively.</p>
      <br>
      <p><strong>Input Panorama</strong>
        &emsp;&emsp;&emsp;&emsp;
        &emsp;&emsp;&emsp;&emsp;
        &emsp;&emsp;&emsp;&emsp;
        &emsp;&emsp;&emsp;&emsp;
        &emsp;
        <strong>Our Predictions</strong>
        &emsp;&emsp;&emsp;&emsp;
        &emsp;&emsp;
      </p>
      <img src="./static/images/bad24.png" width="36%"> <img src="./static/images/bad24.gif" width="54%">
      <img src="./static/images/f7ab.png" width="36%"> <img src="./static/images/f7ab.gif" width="54%">
      <img src="./static/images/85824.png" width="36%"> <img src="./static/images/85824.gif" width="54%">
    </div>
  </section>

  <!-- Abstract part -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Inherent ambiguity in layout annotations poses significant challenges to developing accurate 360° room layout estimation models.
               To address this issue, we propose a novel Bi-Layout model capable of predicting two distinct layout types. 
               One stops at ambiguous regions, while the other extends to encompass all visible areas. Our model employs two global context embeddings,
                where each embedding is designed to capture specific contextual information for each layout type. With our novel feature guidance module, 
                the image feature retrieves relevant context from these embeddings, generating layout-aware features for precise bi-layout predictions. 
            </p>
            <p>
              A unique property of our Bi-Layout model is its ability to inherently detect ambiguous regions by comparing the two predictions. 
                To circumvent the need for manual correction of ambiguous annotations during testing, we also introduce a new metric for disambiguating ground truth layouts. 
                Our method demonstrates superior performance on benchmark datasets, notably outperforming leading approaches. Specifically, on the MatterportLayout dataset, 
                it improves 3DIoU from <strong>81.70%</strong> to <strong>82.57%</strong> across the full test set and notably from <strong>54.80%</strong> to <strong>59.97%</strong> in subsets with significant ambiguity.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/pgR-IgOcNgg?si=BWgnT2LeL7cX_Guh" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>


  <!-- Teaser part -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <!-- Teaser -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Motivation</h2>
          <h2 class="content has-text-justified">
            <p>
              Prior works generate the single layout prediction from the panorama image. 
              Given the single panorama as the input, the state-of-the-art method only generates a single prediction, 
              which faces wrong prediction parts inside the white box. This challenge comes from the inherent ambiguity region in the dataset label, 
              which is caused by inconsistent annotation strategy and is hard to solve by a single prediction method. 
            </p>
            <p>
              The white box indicates the ambiguous regions and the prediction from the state-of-the-art methods are struggled in these regions. 
              We further define the two types of ground truth annotations, enclosed and extended.  
              The enclosed annotation encloses the nearest room.  
              The extended annotation extends to all visible areas. 
              With two label definitions, our model has a clear target to learn and predict.
            </p>
          </h2>
          <img src="./static/images/bi_layout_motivation.png" width="100%">
          <h2 class="content has-text-justified">
            <p>
              <strong>Inherent ambiguity in the MatterportLayout Dataset.</strong> <strong><font color="Blue">Blue</font></strong> and <strong><font color="#82FF82">Green</font></strong> represent ground truth annotations and predictions from the SoTA models, respectively. 
              The layout boundaries are shown on the left, and their bird's-eye view projections are on the right. 
              We define two types of layout annotation: (a) <strong>enclosed type</strong> encloses the room. (b) <strong>extended type</strong> extends to all visible areas. 
              The dashed lines underscore the ambiguity in the dataset labels.
            </p>
          </h2>
          <img src="./static/images/bi_layout_result_for _motivation.png" width="100%">
          <h2 class="content has-text-justified">
            <p>
              We propose our <strong>Bi-Layout model</strong>, which can generate <strong>two distinct</strong> layouts from a single panorama. 
              Within these two layout predictions, we can find a prediction that best fits the dataset label to obtain better performance and solve the ambiguity issue inherent in the dataset label.
            </p>
          </h2>
        </div>
      </div>
      <!-- Teaser -->
  </section>

  <!-- Network Architecture part -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Network Architecture part -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Method</h2>
          <img src="./static/images/bi_layout_network.png" width="100%">
          <h2 class="content has-text-justified">
            <p>
            We proposed three main parts to construct our bi-layout model.
            </p>
            <p>
              <strong>(a)</strong> The first part is the <strong>Feature Extractor</strong> to encode the panorama. 
              We use ResNet to generate different levels of features and use our simplified height compression module 
              to compress and concatenate these features into an efficient 1-dimensional feature representation.
            </p>
            <p>
              <strong>(b)</strong> The second part is our <strong>Global Context Embeddings</strong>, which learn the global contextual information from the dataset labels. 
              These embeddings learn the contextual information during training time.
            </p>
            <p>
              <strong>(c)</strong> The third part is our <strong>Shared Feature Guidance Module</strong>, 
              which guides the shared panorama feature with corresponding global context embedding and generates the final feature for different predictions. 
              This is the most important part of our model design to generate the two distinct layout predictions.
            </p>
          </h2>
        </div>
      </div>
      <!-- Network Architecture part -->
  </section>


  <!-- results part -->
  <section class="section">
    <div class="container is-max-desktop">
      <!-- results part -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Qantitative Comparison</h2>
          <img src="./static/images/quantitative_evaluation.png" width="100%">
          <h2 class="content has-text-justified">
            <p>
              <strong>Full set and Subset evaluation.</strong> 
              <strong>Equivalent branch</strong> represents the output, which is trained with the same label as baseline methods.
              <strong>Disambiguate</strong> is our proposed metric.
            </p>
          </h2>
          <h2 class="title is-3">Qualitative Comparison</h2>
          <img src="./static/images/matterport.png" width="100%">
          <img src="./static/images/zind.png" width="100%">
          <h2 class="content has-text-justified">
            <p>
              <strong>Qualitative comparison on the MatterportLayout (top) and  ZInd datasets (bottom).</strong> 
              <strong><font color="Blue">Blue</font></strong> and <strong><font color="#82FF82">Green</font></strong> represent ground truth labels and predictions, respectively. 
              The boundaries of the room layout are on the left, and their bird's eye view projections are on the right. 
              We show our <strong>disambiguate</strong> results, which effectively address the ambiguity issue, while the SoTA methods struggle with the ambiguity, as highlighted in dashed lines.
            </p>
          </h2>
        </div>
      </div>
      <!-- results part -->
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{tsai2024no,
        title={No more ambiguity in 360◦ room layout via bi-layout estimation},
        author={Tsai, Yu-Ju and Jhang, Jin-Cheng and Zheng, Jingjing and Wang, Wei and Chen, Albert and Sun, Min and Kuo, Cheng-Hao and Yang, Ming-Hsuan},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        year={2024}
      }</code></pre>
    </div>
  </section>


  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">References</h2>

          <div class="content has-text-justified">
            <p>
              [BANMo] Gengshan Yang, et al. “Building animatable 3d neural models from many casual videos.” CVPR. 2022.
            </p>
          </div>
        </div>
      </div>

    </div>
  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://liagm.github.io/" class="external-link" disabled target="_blank">
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This webpage template is adapted from <a href="https://github.com/nerfies/nerfies.github.io"
                target="_blank">Nerfies</a>,
              under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA
                4.0 License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <script src="static/js/script.js"></script>

  <script>
    new BeforeAfter({
      id: '#bear-inst0-color'
    });
    new BeforeAfter({
      id: '#bear-inst0-depth'
    });
    new BeforeAfter({
      id: '#bird-inst1-color'
    });
    new BeforeAfter({
      id: '#bird-inst1-depth'
    });
    // new BeforeAfter({
    //   id: '#cat-pikachu-color'
    // });
    // new BeforeAfter({
    //   id: '#cat-pikachu-depth'
    // });
    // new BeforeAfter({
    //   id: '#cow-inst0-color'
    // });
    // new BeforeAfter({
    //   id: '#cow-inst0-depth'
    // });
    // new BeforeAfter({
    //   id: '#dog-inst0-color'
    // });
    // new BeforeAfter({
    //   id: '#dog-inst0-depth'
    // });
    // new BeforeAfter({
    //   id: '#elephant-inst0-color'
    // });
    // new BeforeAfter({
    //   id: '#elephant-inst0-depth'
    // });
    // new BeforeAfter({
    //   id: '#parrot-yum-color'
    // });
    // new BeforeAfter({
    //   id: '#parrot-yum-depth'
    // });
    // new BeforeAfter({
    //   id: '#polarbear-inst0-color'
    // });
    // new BeforeAfter({
    //   id: '#polarbear-inst0-depth'
    // });
    // new BeforeAfter({
    //   id: '#rabbit-ninja-color'
    // });
    // new BeforeAfter({
    //   id: '#rabbit-ninja-depth'
    // });
    // new BeforeAfter({
    //   id: '#turtle-inst0-color'
    // });
    // new BeforeAfter({
    //   id: '#turtle-inst0-depth'
    // });
    // new BeforeAfter({
    //   id: '#bear-inst0-00206'
    // });
    // new BeforeAfter({
    //   id: '#parrot-yum-00000'
    // });
    // new BeforeAfter({
    //   id: '#elephant-inst0-00033'
    // });
    // new BeforeAfter({
    //   id: '#rabbit-ninja-00121'
    // });
    // new BeforeAfter({
    //   id: '#cockatiel-inst0-00039'
    // });
    // new BeforeAfter({
    //   id: '#cat-pikachu-00000'
    // });

  </script>

</body>

</html>